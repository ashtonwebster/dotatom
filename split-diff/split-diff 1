---
title: A Comparison of Transfer Learning Algorithms in Defect and Vulnerability Detection
author: Ashton Webster
output: pdf_document
abstract: |
    Defect detection using software metrics and text mining has been repeatedly performed with great success.  However, the field of vulnerability detection using similar methods is still fledgling and not fully explored.  The present work is intended to discover if the methods of transfer learning and defect detection can be extended into the domain of vulnerability detection.  Several state of the art transfer learning methods are implemented (many of which are tested in the field of defect prediction) and compared based on their ability to detect cross project vulnerabilities.
---

## Introduction

Consider the following scenario: A manager conducts code reviews to  detect vulnerabilities in a critical application.  The manager can instruct her code reviewers to look at a portion of the available code base and look for potential vulnerabilities on a per-file basis.  The goal is to minimize the amount of time and effort spent by the reviewers and maximize the amount of vulnerabilities found.   Furthermore, assume that there are other existing projects with labeled vulnerabilities on a per-file basis.  Software metrics or language-specific tokens are also available with each file from the *source projects*, as well as a label (defective or not defective, vulnerable or not vulnerable). Is it possible to use this information on the current *target project*?

It turns out that this scenario is an excellent candidate for the application of *Transfer Learning*.  According to *The Handbook of Research on Machine Learning Applications*, Transfer Learning is defined as the "the improvement of learning in a new task through the transfer of knowledge from a related task that has already been learned" [@Olivas2010].  Essentially, the idea is to use existing collections of labeled vulnerability or defect datasets and extend the learned rules to new projects. Many methods have been proposed, including weighting, filtering, and ensemble methods, but it is not clear which methods provide the greatest benefits in terms of performance.

Ensemble methods are one popular idea in the field of transfer learning for defect and vulnerability detection.  These methods create multiple "weak learners" and use different voting, averaging, or clustering methods to produce predictions.  One method is the "cluster then label" idea, which involves creating groups of data around test instances, training classifiers within each group, and then labeling the instances using the multiple created models.  For example, Menzies et. al. propose a two-step process consisting of a clustering function named WHERE and a rule-learning function named WHICH, focusing on  human-readable and understandable output [@Menzies2013a].  Additionally,  Kamishima,  Hamasaki, and Akaho use a modified bagging algorithm called TrBagg, which creates many weak learners by resampling the dataset [@Kamishima2009].  Several methods are proposed on how to recombine the predictions of the individual classifiers generated by TrBagg to produce very accurate predictions in different contexts.

Other methods use a filtering methodology, using only "relevant" training examples. Turhan proposed the "Burak" (Nearest Neighbors) algorithm [@Turhan2009], which finds the closest training instances (from any project) to each test instance.  In [@Peters2013], this method is compared to the "Peters" filter, which flips the idea and focuses on finding the closest test instance to each training instance. Fukushima, Kamei, and McIntosh consider defect introducing changes and how similar correlation between predictors and dependent variables among projects affects cross-project defect prediction performance [@Fukushima2014].  Although it remains rare to see studies analyzing the performance of vulnerability detection, Yamaguchi, Lindner, and Rieck use PCA to find the instances most similar to vulnerable files and flags these for inspection [@Yamaguchi2011].  

Meanwhile, other researchers have focused on normalization methods to bring test and training projects into a similarly distributed feature space.  Name, Pan, and Kim use data from different projects is normalized using a process called TCA+ (improved transfer component analysis).  Minku, Sarro, Mendes, and Ferrucci suggest a method called Dycom which is used which to scale effort estimations from one project and apply them to other projects [@Minku2015].  Another technique, somewhat similar to normalization, is weighting training instances based on similarity to test instances.  Ma, Luo, Zeng, and Chena found this method helpful in creating priors for the Naive Bayes classifier from other source projects and using them to predict on target projects.

The primary research questions we seek to answer are as follows:

* *RQ1: Does the performance of transfer learning methods outperform baseline classifiers?*
* *RQ2: How does the input format (metrics or tokens) impact the performance of vulnerability detection using transfer learning algorithms?*

## Datasets

Two publicly available datasets were used for this study.  First, we examine the software defect dataset available from the PROMISE repository [@Jureczko2010a], which contains 33 projects.  Some projects had multiple versions, but these were grouped under the same project.  This dataset included 20 different software engineering metrics per file (such as lack of cohesion and average cyclomatic complexity) in addition to the number of defects in the file.  The number of defects per file was changed to be a binary field such that the value was zero if no defects were present and one otherwise.  In all, this dataset contains 87,399 files of which 14,623 contained at least one defect.  The purpose of the PROMISE dataset was to evaluate the transfer learning algorithms on a well-known dataset with many previous studies, including [@Peters2013], [@He2012], [@Ma2012], [@Bettenburg2012], and [@Kocaguneli2010].

The second dataset used is a publicly available dataset on PHP code created by [@Walden2014] which contains information on security vulnerabilities as indicated by the National Vulnerability Database [@NIST] and security announcements from project specific pages.  Three projects were considered: Moodle, PHPMyAdmin, and Drupal, all of which are open source and web based.  Both  software engineering metrics (though not the same as the PROMISE dataset) and tokens (vectors indicating which PHP language tokens were present) were available for each file.  In total, there were 3,465 files with 113 vulnerabilities.  Note that the distribution of vulnerabilities across the projects was far from uniform. In particular, Moodle had the lowest frequency of vulnerabilities, while Drupal had the highest.  See figure X for more details.

![Comparison of project vulnerability proportions](images/projvsvuln.png)


## Performance Metrics

There are many available metrics for defect prediction which may extend to vulnerability detection.  Accuracy, although simple, is not frequently used due to class skew (i.e. the number of vulnerable or defective instances is far outnumbered by the number of vulnerable instances). For example, simply labeling all Moodle instances as benign (no vulnerabilities) will result in an accuracy of 99.16% percent, because only 0.84% of the total files are vulnerable.  Clearly, this is not a good estimate of the performance of the classifier.

One of the most popular selection for measuring performance in defect prediction is recall; that is, the proportion correctly identified positive instances (i.e. defects, vulnerabilities) detected out of all true positive instances.  The complimentary measure, *precision*, is often used to assess the proportion of identified positive instances that are actually positive instances.  F-measure (also known as F1 Score or F Score) is also frequently used, which incorporates the geometric mean of precision and sensitivity.  However, Menzies suggests that when the number of positive instances is relatively small compared to total instances, the variability of precision measures results in instabilities in the measure, and can often make the precision (and in turn, F-measure) performance metrics useless [@Menzies2007].  In [@Peters2013], the F-measure is therefore replaced with the G-measure, which removes the precision have attempted to use the G-measure, which replaces precision with one minus the false positive rate.

For this study, we use a measure proposed by Stuckman in [@StuckmanThesis], who suggests the measure Area Under Cost Efficiency Curve (AUCEC).  The curve is given as follows:

![Diagram illustrating the AUCEC measure](images/AUCEC50-curve.png)

Where recall is given as:

$R = \frac{TP}{TP + FN}$

"Inspection Ratio" is given as:

$IR = \frac{TP+FP}{TP + FP + FN + TN}$

Intuitively, $IR$ represents the proportion of total files in the project that would have to be inspected in order to find the positive instances identified by the model.

Often, the AUCEC50 measure is used instead of AUCEC, which measures only the area under the curve up to 50% inspection ratio.  This captures the most "useful" part of the classification, because most companies are not interested in inspecting more than 50% of the codebase in order to find vulnerabilities.  Of course, this is somewhat arbitrary, but in general, the most useful benefits from the classifier turn up when inspecting a relatively small proportion of the code.

## Algorithms Impelemented

Four transfer learning algorithms are implemented and tested in this study and compared to two baselines.  The selection of transfer learning algorithms was somewhat arbitrary, but some intuitively beneficial aspects were considered.  Algorithms with a history of success in defect or vulnerability detection were preferred.  Additionally, an attempt was made to select algorithms using disparate techniques.  For example, we have filtering algorithms (Burak and Peters), ensemble methods (TrBagg), and weighting/normalization methods (Gravity Weight).  A brief explanation of the details and implementation for each algorithm follows.

### Burak

This is a pleasingly intuitive transfer learning algorithm, first proposed by Burak Turhan [@Turhan2009].  For every target test instance, the closest $k$ instances from any other project (including the target training data).  In line with the original experiment [@Turhan2009] and the replication [@Peters2013], we set $k = 10$ for our experiment.  This method is also known as the "Nearest Neighbors" method, for obvious reasons.

Because this method requires computing the distance from every instance to every instance, it can be very slow.  An approximation proposed by Menzies (and implemented in the library) is to first run k-means to create "batches" of instances close to one another and then the Burak Method within each batch [@Peters2013].

### Peters

Building off of the Burak filter, [@Peters2013] proposes the "Peters" filter and demonstrates the benefits of letting training data drive the process of transfer learning.  Instead of relying on test instances, the focus is on training instances.   

1) For each train instance, find closest test instance. In [@Peters2013] this is described as the "biggest fan" of the training instance.
2) For a test instance $x_i$, let the set of training instances be $F_{x_i}$ such that the $x_i$ is the closest test instance for each $f \in F_{x_i}$.  For each test instance $x_i$, find the closest instance $f'_i$ in $F_{x_i}$ and include this in the training set.
3) The final set of $F' = \{f_0 ... f_n\}$ is the filtered training set.

Similar to above, the batching approximation method is used for efficiency.

### TrBagg

There are several different flavors of TrBagg that were introduced by [@Kamishima2009].   The main idea is to take advantage of a machine learning technique known as *bagging*. Training data is repeatedly sampled without replacement to train many *weak classifiers*, which can then be combined in different ways to produce a classification.  The main extension on weak classification contributed by Kamishima is that the weak classifiers are only incorporated to the ensemble if there is evidence to suggest that adding the weak classifier will improve the ability of the overall ensemble to classify instances of the specific target class.  Several heuristic methods are available, but we will use the simplest form, which is simply standard bagging from the entire available training set.

### Gravity Weighting

Ma, Luo, Zeng, and Chena propose a "Transfer Naive Bayes" algorithm which relies on gravitational weighting [@Ma2012]. That is, instances are weighted inversely proportional to their distance from the training instances, based on measure of similarity defined in the paper.  Specifically, Ma uses the weighting for instance as

$$w_i = \frac{s_i}{k - s_i + 1}$$

Where $k$ is the total number of features, and

$$s_i = \sum_{j=1}^k{h(a_{ij})}$$

where

$$h(a_{ij}) = \begin{cases}
1 \mbox{ if } min_j \le a_ij \le max_j\\
0 \mbox{ else}
\end{cases}$$


Although this weighting is proposed for creating a prior distribution for the Naive Bayes classifier, it is in this study used for RandomForest as a parameter to the scikit-learn RandomForest implementation.

### Baselines

There are two baselines which will be used for comparison:

* "Training Baseline": All available source project data is used, with no additional filtering, weighting, or special techniques.
* "Baseline inspection": If we assume a uniform distribution of vulnerabilities throughout the code base, it is expected that looking at $X\%$ of the code will reveal approximately $X\%$ of the vulnerabilities.  This can be thought of as the random approach, where the code reviewer simply inspects files at random.

## Experiment Design

Each experiment consists of assigning one project as the target project and the others as source projects.  Each transfer learning algorithm is applied and evaluated.  For the PHP vulnerability dataset, both metrics and token features are evaluated separately.  Random Forest is used for all classification tasks based on its high performance in a variety of tasks as explained in [@Caruana2006], as well as specifically in defect prediction [@Lessmann2008b].  All transfer learning algorithms were implemented in Python, and the scikit-learn [@scikit-learn] library was used for its implementation of RandomForest and several other convenience functions.  

We begin with a comparison of transfer learning methods in the context of defect prediction using the public PROMISE dataset.  We then move on to a comparison of transfer learning methods for vulnerability detection on the PHP vulnerability dataset.  For the PHP vulnerability dataset, we consider two different input formats for each experiment: metrics and tokens.  Finally, a comparison of how the input format affects on performance is explored.

## Experiment Results

Before beginning the experiments formally, a validation dataset was used which consisted of a 20% random sample of the Drupal dataset.  The reason for this selection was somewhat arbitrary, but the motivating factors were the relatively large proportion of vulnerabilities in the Drupal project. This validation set (which was *not* used in any of the actual experiment test or training sets) was used to tune the parameters for the RandomForest classifier.  Specifically, informal experimentation revealed that increasing the number of trees to 100 (from the default of 20) lead to significant improvement in the RandomForest classification accuracy.

Interestingly, informal experimentation with the TrBagg algorithm revealed that using the Naive method produced the best results for the Drupal algorithm.  Often, using the filtering approaches when bagging resulted in training sets that were too small to be useful.  Therefore, the Naive method explained above was used.  Further research is needed to determine which TrBagg "flavor" performs the best over a larger number of contexts, but from now on in the paper, the reader should assume "TrBagg" refers to the simplistic bootstrapping technique with no additional voting.

For the actual experiments, we show three different figures which show the main comparisons of different methods:

* An example of a single projects AUCEC50 curve, showing the trade off of $IR$ (inspection ratio) and $R$ (recall).
* A chart (first proposed by [@Demsar2006]) comparing the average ranking of the different transfer learning algorithms across all projects
* A table of the top ten classifiers ranked by descending AUCEC50 values, with popular performance metrics such as F-measure, G-measure, recall, precision, accuracy, and several others.

### Promise

![An example of the AUCEC50 curve for the ANT project in the PROMISE dataset.  The tight clustering of lines above the "inspect all baseline" suggests that methods are achieving similar performance metrics.](images/ant.png)

![In the average rank graph with critical differences, we see that TrBag, Source Baseline, and Gravity Weight achieve ranks that are significantly lower than the Burak and Peters filters.](images/promise.png)

![Top 10 Classifiers by Descending AUCEC50 score with selected performance metrics](images/promisetop-aucec50.png)

We begin by reviewing a comparison of transfer learning methods for defect prediction on the common PROMISE dataset as both a baseline and a starting point. By applying the Friedman test, we conclude that the differences in transfer learning methods for the PROMISE dataset are not solely due to chance.  The average rank graph (colored horizontal lines) shows that TrBag, GravityWeight, and the SourceBaseline methods outperform both burak and peters.  

![Comparison of Burak and Peters methods in several performance metrics](images/promise-performancediff.png)

[@Peters2013] assert that the Peters filter outperforms the Burak filter by a median of 40% using the G-measure on the PROMISE dataset. However, this repeated experiement reveals a -2% median difference between Peters and Burak (that is, Peters is 2% *worse* than the Burak filter).  The Peters Filter also outperforms the Burak filter in other measures of success; for example, the median F score for the Peters filter is 2% lower and the AUCEC50 score is about 3 percent lower than the respective scores for the Burak filter.  Furthermore, the Burak *and* Peters filter are both significantly outperformed by the SourceBaseline method (which simply uses all available training data without filtering at all).  Further research is required to determine if other variables intervened to result in inconsistent results between our studies.

### PHP Vulnerabilities

![Here we have an example AUCEC50 curve, which compares the proportion of code inspected and the proportion of total vulnerabilities found.  The methods are closely clustered but all outperform the "inspect all" baseline, which represents code reviews of X% of the code revealing an expected X% of the vulnerabilities.](images/notrain-phpvuln-drupal.png)

![PHP Metrics - average rank.  The average rank of all methods is tightly clustered, indicating that none of the methods is significantly better than others under the friedman test.](images/php-metrics.png)

![PHP Tokens - average rank.  In all cases, we see that none of the average ranks for the methods is significantly better than the others.](images/php-tokens.png)

![Top 10 classifiers by descending AUCEC50](images/phpvuln-aucec50.png)

While there was a significant difference between transfer learning methods in the the PROMISE dataset, the Friedman test determines that the methods applied to the vulnerability dataset did not have a statistically significant difference in performance.  This is confirmed in figure X, where we see that all of the horizontal lines are overlapping and therefore none of the average ranks among the projects is greater than the critical difference necessary for statistical significance.  

There are two possible conclusions based on the results of the Friedman test: either there is no difference between the methods *or* there exists a difference between the methods, but the Friedman test lacks the statistical power necessary to detect the difference. Support for the latter probability can be found in the considerably smaller number of blocks (projects) in the PHP vulnerability dataset, which would drive power down.  Furthermore, statisticians have found that the Friedman test has less power in general than ANOVA tests.  It is worth noting that the number of blocks (projects to test on) in the PHP vulnerability dataset is considerably smaller than the number of projects in the PROMISE dataset, which makes the power even lower.  In [@Zimmerman1993], Zimmerman explains that the Asymptotic Relative Efficiency (ARE, a measure of relative power) reveals that the Friedman test has relatively low power compared to other techniques.  However, the benefit of the Friedman test is that it has very few assumptions (e.g. no normality assumption) and can give results with only a few blocks and groups.

### Comparison Between Tokens and Metrics in the PHP Vulnerability dataset

![Comparison of tokens and metrics in terms of performance in classifying vulnerabilities](images/tokens_metrics_median_diff.png)

Walden, Stuckman, and Scandariato studied the difference between token and metrics in the performance of vulnerability classification on the same PHP vulnerability dataset [@Walden2014] .  They determined that tokens outperformed the metrics for within-project vulnerability detection. However, comparing these input formats for vulnerability classification has not yet been performed in the context of cross-project classification.  

In order to answer this question, the median difference between the AUCEC50 performance metric was calculated for all combinations of block (project) and group (transfer learning method).  The results are shown in Figure X.  In all cases, the metrics slightly outperform the tokens (notice that although the `fp_rate` metric is positive, this indicates that the metrics had a *lower*, and therefore, better, median `fp_rate` than the tokens).  This result suggests that in the context transfer learning, metrics may be more helpful than tokens.  Because this is contrary to the result that [@Walden2014] found, it suggests that the method of machine learning (within project or cross-project) is not independent of the affect of the feature set on the performance.

## Conclusions, Future Work, and Contributions

The comparison of several transfer learning algorithms on the PROMISE dataset leads to several interesting questions.  It is not immediately clear why the results in this study contradict those found by [@Peters2013].  Further research is necessary to explain what factors resulted in different conclusions and if hidden variables are affecting the outcomes.

Additionally, it is interesting to note that while the PROMISE dataset showed statistically significant differences between the performance of the transfer learning algorithms, no significant differences were discovered in the PHP vulnerability dataset.  Due to the aforementioned problems with the power of the Friedman test, it is likely that additional labeled training instances from other projects are required in order to improve the power of the test. Until then, it is not clear whether there exists a difference between transfer learning methods in the context of vulnerability detection.

Let us now address the research question proposed from the introduction:

*RQ1: Does the performance of transfer learning methods outperform baseline classifiers?*

With few exceptions, the SourceBaseline method (simply using all of the data available from other projects with no filtering) appears to perform at least as well as other methods.  In some cases, TrBagg performs at a similar level, but at the expense of significantly more training effort as multiple weak learners must be trained.  This appears to be the case for both defect detection and vulnerability detection.

*RQ2: How does the input format (metrics or tokens) impact the performance of vulnerability detection using transfer learning algorithms?*

Unfortunately, due to the limited amount of data available, it is not possible to conclusively answer this question at this time.  Results from the experiments conducted for this paper suggest that tokens slightly outperform metrics, but no statistical test is performed at this time (perhaps a non-parametric equivalent of a 2-way ANOVA test would be appropriate).

Finally, additional tests are required to extend the research of [@Walden2014] and determine if the benefits of tokens over metrics in vulnerability detection is significant when applying transfer learning methods.  Expanding the quantity and variety of available projects and datasets would prove beneficial in resolving this question as well.  

Contributions of this work are as follows:

* I created an open source implementations of state of the art transfer learning algorithms
* I challenge previous assumptions about defect prediction and vulnerability detection (especially those of Menzies et. al)
* I directly compare and evaluate several transfer learning algorithms from different domains

## Bibliography
