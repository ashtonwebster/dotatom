---
title: A Comparison of Transfer Learning Algorithms in Defect and Vulnerability Detection
author: Ashton Webster
output: pdf_document
abstract: |
    Defect detection using software metrics and text mining has been repeatedly performed with great success.  However, the field of vulnerability detection using similar methods is still fledgling and not fully explored.  The present work is intended to discover if the methods of transfer learning and defect detection can be extended into the domain of vulnerability detection.  Several state of the art transfer learning methods are implemented (many of which are tested in the field of defect prediction) and compared based on their ability to detect cross project vulnerabilities.
---

## Introduction

Consider the following scenario: A manager conducts code reviews to  detect vulnerabilities in a critical application.  The manager can instruct her code reviewers to look at a portion of the available code base and look for potential vulnerabilities on a per-file basis.  The goal is to minimize the amount of time and effort spent by the reviewers and maximize the amount of vulnerabilities found.   Furthermore, assume that there are other existing projects with labeled vulnerabilities on a per-file basis.  Software metrics or language-specific tokens are also available with each file from the *source projects*, as well as a label (defective or not defective, vulnerable or not vulnerable). Is it possible to use this information on the current *target project*?

It turns out that this scenario is an excellent candidate for the application of *Transfer Learning*.  According to *The Handbook of Research on Machine Learning Applications*, Transfer Learning is defined as the "the improvement of learning in a new task through the transfer of knowledge from a related task that has already been learned" [@Olivas2010].  Essentially, the idea is to use existing collections of labeled vulnerability or defect datasets and extend the learned rules to new projects. Many methods have been proposed, including weighting, filtering, and ensemble methods, but it is not clear which methods provide the greatest benefits in terms of performance.

Ensemble methods are one popular idea in the field of transfer learning for defect and vulnerability detection.  These methods create multiple "weak learners" and use different voting, averaging, or clustering methods to produce predictions.  One method is the "cluster then label" idea, which involves creating groups of data around test instances, training classifiers within each group, and then labeling the instances using the multiple created models.  For example, Menzies et. al. propose a two-step process consisting of a clustering function named WHERE and a rule-learning function named WHICH, focusing on  human-readable and understandable output [@Menzies2013a].  Additionally,  Kamishima,  Hamasaki, and Akaho use a modified bagging algorithm called TrBagg, which creates many weak learners by resampling the dataset [@Kamishima2009].  Several methods are proposed on how to recombine the predictions of the individual classifiers generated by TrBagg to produce very accurate predictions in different contexts.

Other methods use a filtering methodology, using only "relevant" training examples. Turhan proposed the "Burak" (Nearest Neighbors) algorithm [@Turhan2009], which finds the closest training instances (from any project) to each test instance.  In [@Peters2013], this method is compared to the "Peters" filter, which flips the idea and focuses on finding the closest test instance to each training instance. Fukushima, Kamei, and McIntosh consider defect introducing changes and how similar correlation between predictors and dependent variables among projects affects cross-project defect prediction performance [@Fukushima2014].  Although it remains rare to see studies analyzing the performance of vulnerability detection, Yamaguchi, Lindner, and Rieck use PCA to find the instances most similar to vulnerable files and flags these for inspection [@Yamaguchi2011].  

Meanwhile, other researchers have focused on normalization methods to bring test and training projects into a similarly distributed feature space.  Name, Pan, and Kim use data from different projects is normalized using a process called TCA+ (improved transfer component analysis).  Minku, Sarro, Mendes, and Ferrucci suggest a method called Dycom which is used which to scale effort estimations from one project and apply them to other projects [@Minku2015].  Another technique, somewhat similar to normalization, is weighting training instances based on similarity to test instances.  Ma, Luo, Zeng, and Chena found this method helpful in creating priors for the Naive Bayes classifier from other source projects and using them to predict on target projects.

The primary research questions we seek to answer are as follows:

* *RQ1: Does the performance of transfer learning methods outperform baseline classifiers?*
* *RQ2: How does the input format (metrics or tokens) impact the performance of vulnerability detection using transfer learning algorithms?*

## Datasets

Two publicly available datasets were used for this study.  First, we examine the software defect dataset available from the PROMISE repository [@Jureczko2010a], which contains 33 projects.  Some projects had multiple versions, but these were grouped under the same project.  This dataset included 20 different software engineering metrics per file (such as lack of cohesion and average cyclomatic complexity) in addition to the number of defects in the file.  The number of defects was changed to be a binary field such that the value was zero if no defects were present and one otherwise.  In all, this dataset contained 87,399 files of which 14,623 contained at least one defect.  The purpose of the PROMISE dataset was to evaluate the transfer learning algorithms on a well-known dataset with many previous studies, including [@Peters2013], [@He2012], [@Ma2012], [@Bettenburg2012] and [@Kocaguneli2010].

The second is a dataset on PHP code created by [@Walden2014], which contains information on security vulnerabilities as indicated by the National Vulnerability Database [@NIST] and security announcements from project-specific pages.  Three open-source, web-based projects were considered: Moodle, phpMyAdmin and Drupal.  Both software engineering metrics (though not the same as the PROMISE dataset) and tokens (vectors indicating which PHP language tokens were present) were available for each file.  In total, there were 3,465 files with 113 vulnerabilities.  Note that the distribution of vulnerabilities across the projects was far from uniform. In particular, Moodle had the lowest frequency of vulnerabilities, while Drupal had the highest.  See figure X for more details.

![Comparison of project vulnerability proportions](images/projvsvuln.png)

## Performance Metrics

There are many available metrics for defect prediction that may extend to vulnerability detection.  Accuracy, although simple, is not frequently used due to class skew (i.e. the number of vulnerable or defective instances is far outnumbered by the number of vulnerable instances). For example, simply labeling all Moodle instances as benign (no vulnerabilities) results in an accuracy of 99.16%, because only 0.84% of the total files are vulnerable.  Clearly, this is not a good estimate of the performance of the classifier.

One of the most popular metrics for measuring performance in defect prediction is *recall*, the proportion of correctly identified positive instances (defects or vulnerabilities) detected out of all true positive instances.  The complimentary measure, *precision*, measures the proportion of identified positive instances that are actually positive instances.  F-measure (also known as F1 Score or F Score), the harmonic mean of precision and recall, is also frequently used.  However, Menzies suggests that when the ratio of positive to total instances is relatively small, the variability of precision measures results in instabilities in the measure, and can often make the precision (and in turn, F-measure) performance metrics useless [@Menzies2007].  In [@Peters2013], the F-measure is therefore replaced with the G-measure, which replaces precision with specificity.

For this study, we use the Area Under Cost Efficiency Curve (AUCEC) metric proposed by Stuckman in [@StuckmanThesis]. The metric is given as follows:

![Diagram illustrating the AUCEC measure](images/AUCEC50-curve.png)

Recall is given as:

$R = \frac{TP}{TP + FN}$

"Inspection Ratio" is given as:

$IR = \frac{TP+FP}{TP + FP + FN + TN}$

Intuitively, $IR$ represents the proportion of files in the project that would have to be inspected in order to find the positive instances identified by the model.

The AUCEC50 measure, which measures only the area under the curve up to an inspection ratio of $50\%$, is often used instead of AUCEC. This measure captures the most useful part of the classification, because most companies are not interested in inspecting more than $50\%$ of the codebase in order to find vulnerabilities.  While the choice of inspection ratio is somewhat arbitrary, in general the most useful benefits from a classifier turn up when inspecting a relatively small proportion of the code.

## Algorithms Impelemented

Four transfer learning algorithms with a history of success in defect or vulnerability prediction were implemented and tested in this study. Algorithms with a history of success in defect and vulnerability detection were preferred. Additionally, an attempt was made to select algorithms using disparate techniques.  For example, we have filtering algorithms (Burak and Peters), ensemble methods (TrBagg), and weighting/normalization methods (Gravity Weight).  A brief explanation of the details and implementation of each algorithm follows.

### Burak

The Burak filter is an intuitive transfer laerning algorithm first proposed by Burak Turhan [@Turhan2009].  For every target test instance, the closest $k$ instances from any other project (including the target training data).  In line with the original experiment [@Turhan2009] and the replication [@Peters2013], we set $k = 10$ for our experiment.  This method is also known as the "Nearest Neighbors" method, for obvious reasons.

Because this method computes the distance from every test instance to every training instance, it can be very slow.  An approximation proposed by Menzies (and implemented in the library) is to first run $k$-means to create batches of instances close to one another, and then run the Burak method within each batch [@Peters2013].

### Peters

Building off of the Burak filter, [@Peters2013] proposes the Peters filter and demonstrates the benefits of letting training data drive the process of transfer learning.  Instead of relying on test instances, the Peters filter focuses on training instances.

1) For each training instance, find the closest test instances. In [@Peters2013], this is called the "biggest fan" of the training instance.
2) For each test instance $x_i$, let the set of training instances $F_{x_i}$ be such that $x_i$ is the closest test instance to each $f \in F_{x_i}$.  For each test instance $x_i$, find the closest instance $f'_i$ in $F_{x_i}$ and include this in the training set.
3) The final set $F' = \{f_0 ... f_n\}$ is the filtered training set.

As above, the batching approximation is used for efficiency..

### TrBagg

[@Kamishima2009] introduces several different flavors of TrBagg.  Each flavor takes advantage of a machine learning techniques known as *bagging*: training data is repeatedly sampled without replacement in order to train many *weak classifiers*; these classifiers can be combined in different ways to produce a classification.  The main extension of weak classification contributed by Kamishima is that weak classifiers are only incorporated in the ensemble if there is evidence to suggest that adding the classifier will improve the ability of the ensemble to classify instances of the specific target class.  Several heuristic methods are available, but we use the simplest one, consisting of standard bagging from the entire available training set.

### Gravity Weighting

Ma, Luo, Zeng and Chena propose a "Transfer Naive Bayes" algorithm, which relies on gravitational weighting [@Ma2012].  That is, instances are weighted inversely proportional to their distance from the training instances based on a similarity metric defined in the paper.  The weighting for instances is given as:

$$w_i = \frac{s_i}{k - s_i + 1}$$

where $k$ is the total number of features and

$$s_i = \sum_{j=1}^k{h(a_{ij})}$$

where

$$h(a_{ij}) = \begin{cases}
1 \mbox{ if } min_j \le a_ij \le max_j\\
0 \mbox{ else}
\end{cases}$$

Although Ma proposes this weighting to create prior distributions for the Naive Bayes Classifier, in this study it is used as a parameter to RandomForest, the scikit-learn implementation of the random forest classifier.

### Baselines

Two baselines were used for comparison:

* "Training Baseline": All available source project data is used, with no additional filtering, weighting or special techniques.
* "Baseline Inspection": If a uniform distribution of vulnerabilities throughout the codebase is assumed, we expect that inspecting $X\%$ of the code will reveal approximately $X\%$ of the vulnerabilities.  This can be thought of as the random approach, where the review simply inspects files at random.

## Experiment Design

For each experiment, one project was assigned as the target project and the others as source projects.  Each transfer learning algorithm was trained on the source projects and evaluated on the target project.  For the PHP vulnerability dataset, metrics and token features were evaluated separately.  The RandomForest classifier was used for all classification tasks based on its high performance at a variety of tasks [@Caruana2006] including defect prediction [@Lessmann2008b].  All transfer learning algorithms were implemented in Python, and the scikit-learn [@scikit-learn] library was used for its implementation of RandomForest.

We begin with a comparison of transfer learning methods in the context of defect prediction using the PROMISE dataset.  Then, we compare the transfer learning methods for vulnerability detection on the PHP vulnerability dataset.  For the PHP vulnerability dataset, we consider two different input formats for each experiment: metrics and tokens.  Finally, we compare the effect of input format on performance.

## Experiment Results

Before beginning the experiments, $20\%$ of the Drupal dataset was randomly sampled for use as a validation set.  The factor motivating this selection was the relatively large proportion of vulnerabilities in the Drupal project.  The validation set, which was *not* used in the experiments' test or training sets, was used to tune the parameters of the RandomForest classifier.  Specifically, informal experimentation revealed that increasing the number of trees to 100 (from the default of 20) significantly improved the classification accuracy of RandomForest.

Interestingly, informal experimentation with the TrBagg algorithm revealed that using the Naive method produced the best results for the Drupal dataset.  Often, using the filtering approaches when bagging resulted in training sets that were too small to be useful.  Therefore, the Naive method outlined above was used.  Further research is needed to determine which TrBagg "flavor" performs the best over a larger number of contexts; from now on, the reader should assume that "TrBagg" refers to the simple bootstrapping technique with no additional voting.

For each experiment, we present three figures that show the main comparisons between the transfer learning methods:

* An example of a single project's AUCEC50 curve, showing the trade-off for $IR$ (inspection ratio) and $R$ (recall).
* A chart (first proposed by [@Demsar2006]) comparing the average ranking of the different transfer learning algorithms across all projects.
* A table of the top 10 classifiers ranked by descending AUCEC50 values, including performance metrics such as F-measure, G-measure, recall, precision and accuracy.

### PROMISE

![An example of the AUCEC50 curve for the ANT project in the PROMISE dataset.  The tight clustering of lines above the "inspect all baseline" suggests that the methods are achieving similar performance metrics.](images/ant.png)

![In the average rank graph with critical differences, we see that TrBag, Source Baseline and Gravity Weight achieve significantly lower ranks than the Burak and Peters filters.](images/promise.png)

![Top 10 Classifiers by Descending AUCEC50 score with selected performance metrics](images/promisetop-aucec50.png)

We begin by comparing the performance of the transfer learning methods for defect prediction on the PROMISE dataset. By applying the Friedman test, we conclude that the difference between the transfer learning methods on the PROMISE dataset are not solely due to chance.  The average rank graph shows that the TrBagg, GravityWeight and SourceBaseline methods outperform the Burak and Peters filters.

![Comparison of Burak and Peters methods by several performance metrics](images/promise-performancediff.png)

[@Peters2013] asserts that the Peters filter outperforms the Burak filter on the PROMISE dataset by a median of $40\%$ using the G-measure.  However, our repetition reveals a $-2\%$ median difference between the Peters and Burak filters (that is, the Peters filter is $2\%$ worse than the Burak filter).  The Peters filter also outperforms the Burak filter in other measures of success.  For example, the median F score for the Peters filter is $2\%$ lower, and the AUCEC50 score is about $3\%$ lower, than the the respective scores for the Burak filter.  Furthermore, the Burak *and* Peters filters are significantly outperformed by the SourceBaseline method (which simply uses all available training data without filtering).  Further research is required to determine whether other variables intervened to result inconsistent results between our studies.

### PHP Vulnerabilities

![Here we have an example AUCEC50 curve, which compares the proportion of code inspected and the proportion of total vulnerabilities found.  The methods are closely clustered but all outperform the "inspect all" baseline, which represents code reviews of X% of the code revealing an expected X% of the vulnerabilities.](images/notrain-phpvuln-drupal.png)

![PHP Metrics - average rank.  The average rank of all methods is tightly clustered, indicating that none of the methods is significantly better than others under the friedman test.](images/php-metrics.png)

![PHP Tokens - average rank.  In all cases, we see that none of the average ranks for the methods is significantly better than the others.](images/php-tokens.png)

![Top 10 classifiers by descending AUCEC50](images/phpvuln-aucec50.png)

While we found a significant difference between the transfer learning methods on the PROMISE dataset, the Friedman test implies that the methods applied to the vulnerability dataset had no statistically significant differences in performance.  This is confirmed by Figure X, where we see that all of the horizontal lines overlap, and therefore none of the average ranks among the projects is greater than the critical difference necessary for statistical significance.

Two possible conclusions can be drawn from the results of the Friedman test: either there is no difference between the methods *or* there exists a difference between the methods, but the Friedman test lacks the statistical power necessary to detect that difference.  Support for the latter hypothesis can be found in the considerably smaller number of blocks (test projects) in the PHP vulnerability dataset, which would decrease the test's power.  Furthermore, statisticians have found that the Friedman test has less power in general than ANOVA tests.  It is worth noting that the number of blocks in the PHP vulnerability dataset is considerably smaller than the number of blocks in the PROMISE dataset, which decreases the power even further.  In [@Zimmerman1993], Zimmerman explains that the Asymptotic Relative Efficiency (ARE, a measure of relative power) reveals that the Friedman test has relatively low power compared to other techniques.  However, the Friedman test has very few assumptions (e.g., no normality assumption) and gives results with only a few blocks and groups.

### Comparison Between Tokens and Metrics in the PHP Vulnerability dataset

![Comparison of tokens and metrics in terms of performance in classifying vulnerabilities](images/tokens_metrics_median_diff.png)

Walden, Stuckman and Scandariato studied the difference between tokens and metrics in the performance of vulnerability classification on the PHP vulnerability dataset [@Walden2014].  They determined that tokens outperformed metrics for within-project vulnerability detection.  However, comparing these input formats for vulnerability detection has not yet been performed in the context of cross-project classification.

In order to answer this question, the median difference between AUCEC50 scores was calculated for all combinations of blocks (projects) and groups (transfer learning methods).  The results are shown in Figure X.  In all cases, the metrics slightly outperform the tokens (although the `fp_rate` metric is positive, this indicates that the metrics had a *lower*, and therefore better, median `fp_rate`).  This result suggests that in the context of transfer learning, metrics may be more helpful than tokens.  Because this contradicts the results of [@Walden2014], it suggests that the performance of machine learning methods (within-project or cross-project) is affected by the feature set.

## Conclusions, Future Work, and Contributions

The comparison of several transfer learning algorithms on the PROMISE dataset leads to several interesting questions.  It is not immediately clear why the results in this study contradict those found by [@Peters2013].  Further research is necessary to explain what factors resulted in different conclusions and if hidden variables are affecting the outcomes.

Additionally, it is interesting to note that while the PROMISE dataset showed statistically significant differences between the performance of the transfer learning algorithms, no significant differences were discovered in the PHP vulnerability dataset.  Due to the aforementioned problems with the power of the Friedman test, it is likely that additional labeled training instances from other projects are required in order to improve the power of the test. Until then, it is not clear whether there exists a difference between transfer learning methods in the context of vulnerability detection.

Let us now address the research question proposed from the introduction:

*RQ1: Does the performance of transfer learning methods outperform baseline classifiers?*

With few exceptions, the SourceBaseline method (simply using all of the data available from other projects with no filtering) appears to perform at least as well as other methods.  In some cases, TrBagg performs at a similar level, but at the expense of significantly more training effort as multiple weak learners must be trained.  This appears to be the case for both defect detection and vulnerability detection.

*RQ2: How does the input format (metrics or tokens) impact the performance of vulnerability detection using transfer learning algorithms?*

Unfortunately, due to the limited amount of data available, it is not possible to conclusively answer this question at this time.  Results from the experiments conducted for this paper suggest that tokens slightly outperform metrics, but no statistical test is performed at this time (perhaps a non-parametric equivalent of a 2-way ANOVA test would be appropriate).

Finally, additional tests are required to extend the research of [@Walden2014] and determine if the benefits of tokens over metrics in vulnerability detection is significant when applying transfer learning methods.  Expanding the quantity and variety of available projects and datasets would prove beneficial in resolving this question as well.  

Contributions of this work are as follows:

* I created an open source implementations of state of the art transfer learning algorithms
* I challenge previous assumptions about defect prediction and vulnerability detection (especially those of Menzies et. al)
* I directly compare and evaluate several transfer learning algorithms from different domains

## Bibliography
